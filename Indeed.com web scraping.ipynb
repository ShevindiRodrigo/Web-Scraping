{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b8f982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c25765f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HEADERS = {\"User-Agent\" : \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Mobile Safari/537.36\"}\n",
    "\n",
    "##url = \"https://api.scrapingdog.com/scrape?api_key=64a75ff5377778182af3780e&url=https://au.indeed.com/jobs?q=data+analyst&l=Australia&from=searchOnHP&vjk=1ffb7092c060c160&dynamic=false\"\n",
    "#req = requests.get(url, headers = HEADERS)\n",
    "\n",
    "#soup = BeautifulSoup(req.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968799ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_list = soup.find(\"ul\",{\"class\":\"jobsearch-ResultsList css-0\"})\n",
    "\n",
    "urls =[]\n",
    "for elt in all_list:\n",
    "    a = elt.find_all('a',{\"id\":lambda x: x and x.startswith('job_')})\n",
    "    for t in a:\n",
    "        a_tag = t['href']\n",
    "        urls.append(a_tag)\n",
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a26638",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_text = []\n",
    "for elt in all_list:\n",
    "    salary = elt.find_all('div',{\"class\":\"metadata salary-snippet-container\"})\n",
    "    if len(salary)==0:\n",
    "        salary_text.append('None')\n",
    "        \n",
    "    else:\n",
    "        for s in salary:\n",
    "            salary_text.append(t.text)\n",
    "salary_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00559f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "job_title = []\n",
    "Company =[]\n",
    "Location = []\n",
    "Job_desc = []\n",
    "root = \"https://www.indeed.com\"\n",
    "for url in urls:\n",
    "    website = f'{root}/{url}'   \n",
    "    website = \"https://api.scrapingdog.com/scrape?api_key=64a75ff5377778182af3780e&url=\"+website\n",
    "\n",
    "    req = requests.get(website,headers=HEADERS)\n",
    "    soup = BeautifulSoup(req.text,'html.parser')\n",
    "    \n",
    "    title = soup.find('h1')\n",
    "    job_title.append(title.text if title else 'N/A')\n",
    "    \n",
    "    company = soup.find('div',{\"data-testid\":\"inlineHeader-companyName\"})\n",
    "    Company.append(company.text if company else 'N/A')\n",
    "    \n",
    "    location = soup.find('div',{\"data-testid\":\"inlineHeader-companyLocation\"})\n",
    "    Location.append(location.text if location else 'N/A')\n",
    "    \n",
    "    desc = soup.find('div',{\"id\":\"jobDescriptionText\"})\n",
    "    description = desc.find_all('p') if desc else []\n",
    "    if len(description) > 1:\n",
    "        des =\"\"\n",
    "        for txt in description:\n",
    "            des = des + txt.text\n",
    "        Job_desc.append(des)\n",
    "    else:\n",
    "        description = desc.find('p') if desc else []\n",
    "        Job_desc.append(description.text if description else 'N/A')\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78f07ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Job title','Company','Location', 'Salary', 'Description']\n",
    "\n",
    "\n",
    "dict_job = []\n",
    "\n",
    "for i in range(len(job_title)):\n",
    "    job = {\n",
    "        \"Job title\": job_title[i],\n",
    "        \"Company\": Company[i],\n",
    "        \"Location\": Location[i],\n",
    "        \"Salary\": salary_text[i],\n",
    "        \"Description\": Job_desc[i]\n",
    "    }\n",
    "    dict_job.append(job)\n",
    "\n",
    "with open('jobs_data_analyst.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(dict_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca685f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "page_ref = \"\"\n",
    "num = 0\n",
    "url =f'https://au.indeed.com/jobs?q=data+analyst&l=Australia&from=searchOnHP&vjk=1ffb7092c060c160'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30938677",
   "metadata": {},
   "outputs": [],
   "source": [
    "web_pages = []\n",
    "\n",
    "for num in range(0, 110, 10):\n",
    "    if num == 0:\n",
    "        page_ref = \"\"\n",
    "    else:\n",
    "        page_ref = f'&start={num}'\n",
    "    \n",
    "    website = f'https://api.scrapingdog.com/scrape?api_key=64a75ff5377778182af3780e&url={url}{page_ref}'\n",
    "    web_pages.append(website)\n",
    "\n",
    "web_pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc75a346",
   "metadata": {},
   "outputs": [],
   "source": [
    "salary_text = []\n",
    "job_title = []\n",
    "Company =[]\n",
    "Location = []\n",
    "Job_desc = []\n",
    "\n",
    "for link in web_pages:\n",
    "    req = requests.get(link, headers = HEADERS)\n",
    "\n",
    "    soup = BeautifulSoup(req.text,'html.parser')\n",
    "\n",
    "    all_list = soup.find(\"ul\",{\"class\":\"jobsearch-ResultsList css-0\"})\n",
    "\n",
    "    urls =[]\n",
    "    for elt in all_list:\n",
    "        a = elt.find_all('a',{\"id\":lambda x: x and x.startswith('job_')})\n",
    "        for t in a:\n",
    "            a_tag = t['href']\n",
    "            urls.append(a_tag)\n",
    "\n",
    "    \n",
    "    for elt in all_list:\n",
    "        salary = elt.find_all('div',{\"class\":\"metadata salary-snippet-container\"})\n",
    "        if len(salary)==0:\n",
    "            salary_text.append('None')\n",
    "\n",
    "        else:\n",
    "            for s in salary:\n",
    "                salary_text.append(t.text)\n",
    "\n",
    "    root = \"https://www.indeed.com\"\n",
    "    for url in urls:\n",
    "        website = f'{root}/{url}'   \n",
    "        website = \"https://api.scrapingdog.com/scrape?api_key=64a75ff5377778182af3780e&url=\"+website\n",
    "\n",
    "        req = requests.get(website,headers=HEADERS)\n",
    "        soup = BeautifulSoup(req.text,'html.parser')\n",
    "\n",
    "        title = soup.find('h1')\n",
    "        job_title.append(title.text if title else 'N/A')\n",
    "\n",
    "        company = soup.find('div',{\"data-testid\":\"inlineHeader-companyName\"})\n",
    "        Company.append(company.text if company else 'N/A')\n",
    "\n",
    "        location = soup.find('div',{\"data-testid\":\"inlineHeader-companyLocation\"})\n",
    "        Location.append(location.text if location else 'N/A')\n",
    "\n",
    "        desc = soup.find('div',{\"id\":\"jobDescriptionText\"})\n",
    "        description = desc.find_all('p') if desc else []\n",
    "        if len(description) > 1:\n",
    "            des =\"\"\n",
    "            for txt in description:\n",
    "                des = des + txt.text\n",
    "            Job_desc.append(des)\n",
    "        else:\n",
    "            description = desc.find('p') if desc else []\n",
    "            Job_desc.append(description.text if description else 'N/A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098ce4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = ['Job title','Company','Location', 'Salary', 'Description']\n",
    "\n",
    "\n",
    "dict_job = []\n",
    "\n",
    "for i in range(len(job_title)):\n",
    "    job = {\n",
    "        \"Job title\": job_title[i],\n",
    "        \"Company\": Company[i],\n",
    "        \"Location\": Location[i],\n",
    "        \"Salary\": salary_text[i],\n",
    "        \"Description\": Job_desc[i]\n",
    "    }\n",
    "    dict_job.append(job)\n",
    "\n",
    "with open('jobs_data_analyst.csv', 'w', encoding='UTF8', newline='') as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=header)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(dict_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd989dc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_job = pd.read_csv(\"jobs_data_analyst.csv\", na_values=\"?\")\n",
    "df_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5228df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f030a9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job.isnull().any(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cb212d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job['Job title'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb06f2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job.dropna(subset = ['Job title'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266c8a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab86fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job['Job title'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859ceaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job['Description'].isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c3e041",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job.dropna(subset = ['Description'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1840a867",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfad4b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7ae590",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job = df_job[['Job title','Company','Location','Description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1fe99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fcf114",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d82c6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job =df_job.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3744df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723420bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = str(df_job[\"Description\"][0])\n",
    "lower_text = text.lower()\n",
    "lower_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f646862",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --user -U nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f70859",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c15b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "token_text = word_tokenize(lower_text)\n",
    "token_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599887da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punctuation_list_new = list(string.punctuation)\n",
    "manual_list_of_punctuation = ['’','``','\"\"','_']\n",
    "for x in manual_list_of_punctuation:\n",
    "    punctuation_list_new.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142c74dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punct_txt = []\n",
    "for elt in token_text:\n",
    "    if elt not in punctuation_list_new:\n",
    "        no_punct_txt.append(elt)\n",
    "no_punct_txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f626a07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "filtered_list = []\n",
    "for elt in no_punct_txt:\n",
    "    if elt not in stop_words:\n",
    "        filtered_list.append(elt)\n",
    "filtered_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b794104b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "lemmatize_list = [lemmatizer.lemmatize(w)for w in filtered_list]\n",
    "lemmatize_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2736d321",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lemmatize_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85989796",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "stemmed_list = [ps.stem(w) for w in lemmatize_list]\n",
    "stemmed_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4842c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from nltk import ngrams\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the frequency of each word using Counter\n",
    "word_frq = Counter(stemmed_list)\n",
    "#for word, freq in word_frq.items():\n",
    "   # print(f\"{word}: {freq}\")\n",
    "\n",
    "paired_freq = Counter(ngrams(stemmed_list,2))\n",
    "trigram = Counter(ngrams(stemmed_list,3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0a1256",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq = pd.DataFrame(word_frq.items(),columns=['word','frequency']).sort_values(by='frequency',ascending=False)\n",
    "\n",
    "word_pairs =pd.DataFrame(paired_freq.items(),columns=['pairs','frequency']).sort_values(by='frequency',ascending=False)\n",
    "\n",
    "trigrams =pd.DataFrame(trigram.items(),columns=['trigrams','frequency']).sort_values(by='frequency',ascending=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bbfe0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(3,1,figsize=(8,20))\n",
    "sns.barplot(ax=axes[0],x='frequency',y='word',data=word_freq.head(30))\n",
    "sns.barplot(ax=axes[1],x='frequency',y='pairs',data=word_pairs.head(30))\n",
    "sns.barplot(ax=axes[2],x='frequency',y='trigrams',data=trigrams.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27dcdb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq = nltk.FreqDist(stemmed_list)\n",
    "freq.plot(20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
